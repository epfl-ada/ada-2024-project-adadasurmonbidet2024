{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.utils.results_utils import *\n",
    "from src.utils.data_utils import str_dict_to_values\n",
    "from src.utils.results_utils import *\n",
    "from src.utils.ml_utils import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "English-Speaking               2812\n",
       "Germanic                       2359\n",
       "Romance                        1847\n",
       "Eastern Slavic                 1749\n",
       "Southern and Western Slavic    1286\n",
       "Hispanic                        891\n",
       "Nordic                          882\n",
       "East Asian                      673\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and present the data\n",
    "df_ethnicity = pd.read_csv('data/name_ethnicity.csv')\n",
    "df_ethnicity['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the most frequent special character to the alphabet: 'é', 'è', 'á' and 'í'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_alphabet = 'abcdefghijklmnopqrstuvwxyzéèíá'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vaclav</td>\n",
       "      <td>Southern and Western Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allan</td>\n",
       "      <td>Eastern Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kristine</td>\n",
       "      <td>Nordic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matteo</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Isao</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name                      Country\n",
       "0    Vaclav  Southern and Western Slavic\n",
       "1     Allan               Eastern Slavic\n",
       "2  Kristine                       Nordic\n",
       "3    Matteo                      Romance\n",
       "4      Isao                   East Asian"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ethnicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>name_length</th>\n",
       "      <th>vowel_count</th>\n",
       "      <th>consonant_count</th>\n",
       "      <th>a_f</th>\n",
       "      <th>b_f</th>\n",
       "      <th>c_f</th>\n",
       "      <th>d_f</th>\n",
       "      <th>e_f</th>\n",
       "      <th>...</th>\n",
       "      <th>u_l</th>\n",
       "      <th>v_l</th>\n",
       "      <th>w_l</th>\n",
       "      <th>x_l</th>\n",
       "      <th>y_l</th>\n",
       "      <th>z_l</th>\n",
       "      <th>é_l</th>\n",
       "      <th>è_l</th>\n",
       "      <th>í_l</th>\n",
       "      <th>á_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vaclav</td>\n",
       "      <td>Southern and Western Slavic</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allan</td>\n",
       "      <td>Eastern Slavic</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kristine</td>\n",
       "      <td>Nordic</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matteo</td>\n",
       "      <td>Romance</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Isao</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name                      Country  name_length  vowel_count  \\\n",
       "0    Vaclav  Southern and Western Slavic            6            2   \n",
       "1     Allan               Eastern Slavic            5            2   \n",
       "2  Kristine                       Nordic            8            3   \n",
       "3    Matteo                      Romance            6            3   \n",
       "4      Isao                   East Asian            4            3   \n",
       "\n",
       "   consonant_count  a_f  b_f  c_f  d_f  e_f  ...  u_l  v_l  w_l  x_l  y_l  \\\n",
       "0                4    0    0    0    0    0  ...    0    1    0    0    0   \n",
       "1                3    1    0    0    0    0  ...    0    0    0    0    0   \n",
       "2                5    0    0    0    0    0  ...    0    0    0    0    0   \n",
       "3                3    0    0    0    0    0  ...    0    0    0    0    0   \n",
       "4                1    0    0    0    0    0  ...    0    0    0    0    0   \n",
       "\n",
       "   z_l  é_l  è_l  í_l  á_l  \n",
       "0    0    0    0    0    0  \n",
       "1    0    0    0    0    0  \n",
       "2    0    0    0    0    0  \n",
       "3    0    0    0    0    0  \n",
       "4    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_processor = NameFeatureProcessor('Name', ngram_range = (2,3))\n",
    "\n",
    "df_ml = origin_processor.process(df_ethnicity,alphabet = augmented_alphabet,analyze_name = True, diacritic = False, phonetics = False, first_last = True, ngram=False)\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(analyzer='char', ngram_range=(2, 3), n_features=1000)  \n",
    "ngram_features = vectorizer.fit_transform(df_ml['Name'])\n",
    "n_gram_df = pd.DataFrame(ngram_features.toarray())\n",
    "df_ml = pd.concat([df_ml, n_gram_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hashing_vectorizer_origin.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorModel_o():\n",
    "    def __init__(self,df:pd.DataFrame,feature:str):\n",
    "        self.df = df\n",
    "        #Feature we want to predict\n",
    "        self.feature = feature\n",
    "\n",
    "    def train(self,df, balancing=False):\n",
    "        X = df.drop(columns=[self.feature])\n",
    "        y = df[self.feature]\n",
    "\n",
    "        #Create the train and validation set\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        if balancing:\n",
    "            smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        #Training\n",
    "        model = MLPClassifier(solver='adam',alpha=10**-5, hidden_layer_sizes=(10,10,2), max_iter=300, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        #Store the model to avoid recomputing\n",
    "        with open(f'model_{self.feature}.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        #Print report\n",
    "        y_pred = model.predict(X_val)\n",
    "        print(classification_report(y_val,y_pred))\n",
    "\n",
    "    \n",
    "    def feature_creation(self,name):\n",
    "        augmented_alphabet = 'abcdefghijklmnopqrstuvwxyzéèíá'\n",
    "\n",
    "        df_pred = pd.DataFrame([name], columns=['Name'])\n",
    "        pred_processor = NameFeatureProcessor('Name',ngram_range=(2,3))\n",
    "        df_pred =pred_processor.process(df_pred,alphabet = augmented_alphabet,analyze_name = True, diacritic = False, phonetics = False, first_last = True, ngram=False)\n",
    "\n",
    "        with open(f'hashing_vectorizer_origin.pkl', 'rb') as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "\n",
    "        ngram_name = vectorizer.transform(df_pred['Name'])\n",
    "        ngram_name_df = pd.DataFrame(ngram_name.toarray())\n",
    "        df_pred = pd.concat([df_pred, ngram_name_df], axis=1)\n",
    "        return df_pred\n",
    "\n",
    "    def predict_one(self,df):\n",
    "        #Load the model\n",
    "        with open(f'model_{self.feature}.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        df.drop(columns=['Name'],inplace=True)\n",
    "        df.columns = df.columns.astype(str)\n",
    "        return model.predict(df)\n",
    "    \n",
    "    def create_and_predict(self,name):\n",
    "        df = self.feature_creation(name)\n",
    "        pred = self.predict_one(df)\n",
    "        print('Prediction: ',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_predictor = PredictorModel_o(df_ml,'Country')\n",
    "df_origin = df_ml.drop(columns='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "rows_with_nans = df_origin.isna().any(axis=1).sum()\n",
    "print(rows_with_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the rows with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = df_origin.dropna()\n",
    "df_origin.columns = df_origin.columns.astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 East Asian       0.79      0.86      0.82        64\n",
      "             Eastern Slavic       0.75      0.40      0.52       194\n",
      "           English-Speaking       0.65      0.45      0.53       315\n",
      "                   Germanic       0.46      0.55      0.50       224\n",
      "                   Hispanic       0.55      0.47      0.51        93\n",
      "                     Nordic       0.47      0.56      0.51        71\n",
      "                    Romance       0.64      0.63      0.63       178\n",
      "Southern and Western Slavic       0.34      0.77      0.47       111\n",
      "\n",
      "                   accuracy                           0.54      1250\n",
      "                  macro avg       0.58      0.59      0.56      1250\n",
      "               weighted avg       0.59      0.54      0.54      1250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amaur\\anaconda3\\envs\\ada\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "origin_predictor.train(df_origin, balancing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  ['Germanic']\n"
     ]
    }
   ],
   "source": [
    "origin_predictor.create_and_predict('John')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
