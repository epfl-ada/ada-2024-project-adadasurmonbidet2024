{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\amaur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\amaur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from src.utils.results_utils import *\n",
    "from src.utils.data_utils import str_dict_to_values\n",
    "from src.utils.results_utils import *\n",
    "from src.utils.ml_utils import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "English-Speaking               2812\n",
       "Germanic                       2359\n",
       "Romance                        1847\n",
       "Eastern Slavic                 1749\n",
       "Southern and Western Slavic    1286\n",
       "Hispanic                        891\n",
       "Nordic                          882\n",
       "East Asian                      673\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and present the data\n",
    "df_ethnicity = pd.read_csv('data/name_ethnicity.csv')\n",
    "df_ethnicity['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the most frequent special character to the alphabet: 'é', 'è', 'á' and 'í'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_alphabet = 'abcdefghijklmnopqrstuvwxyzéèíá'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vaclav</td>\n",
       "      <td>Southern and Western Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allan</td>\n",
       "      <td>Eastern Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kristine</td>\n",
       "      <td>Nordic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matteo</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Isao</td>\n",
       "      <td>East Asian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name                      Country\n",
       "0    Vaclav  Southern and Western Slavic\n",
       "1     Allan               Eastern Slavic\n",
       "2  Kristine                       Nordic\n",
       "3    Matteo                      Romance\n",
       "4      Isao                   East Asian"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ethnicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>name_length</th>\n",
       "      <th>vowel_count</th>\n",
       "      <th>consonant_count</th>\n",
       "      <th>a_f</th>\n",
       "      <th>b_f</th>\n",
       "      <th>c_f</th>\n",
       "      <th>d_f</th>\n",
       "      <th>e_f</th>\n",
       "      <th>...</th>\n",
       "      <th>u_l</th>\n",
       "      <th>v_l</th>\n",
       "      <th>w_l</th>\n",
       "      <th>x_l</th>\n",
       "      <th>y_l</th>\n",
       "      <th>z_l</th>\n",
       "      <th>é_l</th>\n",
       "      <th>è_l</th>\n",
       "      <th>í_l</th>\n",
       "      <th>á_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vaclav</td>\n",
       "      <td>Southern and Western Slavic</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allan</td>\n",
       "      <td>Eastern Slavic</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kristine</td>\n",
       "      <td>Nordic</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matteo</td>\n",
       "      <td>Romance</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Isao</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name                      Country  name_length  vowel_count  \\\n",
       "0    Vaclav  Southern and Western Slavic            6            2   \n",
       "1     Allan               Eastern Slavic            5            2   \n",
       "2  Kristine                       Nordic            8            3   \n",
       "3    Matteo                      Romance            6            3   \n",
       "4      Isao                   East Asian            4            3   \n",
       "\n",
       "   consonant_count  a_f  b_f  c_f  d_f  e_f  ...  u_l  v_l  w_l  x_l  y_l  \\\n",
       "0                4    0    0    0    0    0  ...    0    1    0    0    0   \n",
       "1                3    1    0    0    0    0  ...    0    0    0    0    0   \n",
       "2                5    0    0    0    0    0  ...    0    0    0    0    0   \n",
       "3                3    0    0    0    0    0  ...    0    0    0    0    0   \n",
       "4                1    0    0    0    0    0  ...    0    0    0    0    0   \n",
       "\n",
       "   z_l  é_l  è_l  í_l  á_l  \n",
       "0    0    0    0    0    0  \n",
       "1    0    0    0    0    0  \n",
       "2    0    0    0    0    0  \n",
       "3    0    0    0    0    0  \n",
       "4    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_processor = NameFeatureProcessor('Name', ngram_range = (2,3))\n",
    "\n",
    "df_ml = origin_processor.process(df_ethnicity,alphabet = augmented_alphabet,analyze_name = True, diacritic = False, phonetics = False, first_last = True, ngram=False)\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(analyzer='char', ngram_range=(2, 3), n_features=1000)  \n",
    "ngram_features = vectorizer.fit_transform(df_ml['Name'])\n",
    "n_gram_df = pd.DataFrame(ngram_features.toarray())\n",
    "df_ml = pd.concat([df_ml, n_gram_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hashing_vectorizer_origin.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_predictor = PredictorModel_o(df_ml,'Country')\n",
    "df_origin = df_ml.drop(columns='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "rows_with_nans = df_origin.isna().any(axis=1).sum()\n",
    "print(rows_with_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the rows with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = df_origin.dropna()\n",
    "df_origin.columns = df_origin.columns.astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 East Asian       0.79      0.86      0.82        64\n",
      "             Eastern Slavic       0.75      0.40      0.52       194\n",
      "           English-Speaking       0.65      0.45      0.53       315\n",
      "                   Germanic       0.46      0.55      0.50       224\n",
      "                   Hispanic       0.55      0.47      0.51        93\n",
      "                     Nordic       0.47      0.56      0.51        71\n",
      "                    Romance       0.64      0.63      0.63       178\n",
      "Southern and Western Slavic       0.34      0.77      0.47       111\n",
      "\n",
      "                   accuracy                           0.54      1250\n",
      "                  macro avg       0.58      0.59      0.56      1250\n",
      "               weighted avg       0.59      0.54      0.54      1250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amaur\\anaconda3\\envs\\ada\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "origin_predictor.train(df_origin, balancing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  ['Germanic']\n"
     ]
    }
   ],
   "source": [
    "origin_predictor.create_and_predict('John')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for the names of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akooshay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Melanie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jericho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bashira</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name\n",
       "0  Akooshay\n",
       "1   Melanie\n",
       "2  Williams\n",
       "3   Jericho\n",
       "4   Bashira"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names = pd.read_csv('data/cleaned.csv')\n",
    "to_be_dropped = ['Wikipedia_ID','Languages','Country','Name','Sex','Actor_age','Release_date','Genre_Category']\n",
    "df_names = df_names.drop(columns=to_be_dropped)\n",
    "df_names = df_names.head(10000)\n",
    "df_names.rename(columns={'Character_name': 'Name'}, inplace=True)\n",
    "df_names.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_origin = 'model_Country.pkl'\n",
    "\n",
    "with open(path_origin, 'rb') as file:\n",
    "    predict_origin = pickle.load(file)\n",
    "\n",
    "def feature_creation_o(df_pred):\n",
    "    augmented_alphabet = 'abcdefghijklmnopqrstuvwxyzéèíá'\n",
    "    pred_processor = NameFeatureProcessor('Name',ngram_range=(2,3))\n",
    "    df_pred =pred_processor.process(df_pred,alphabet = augmented_alphabet,analyze_name = True, diacritic = False, phonetics = False, first_last = True, ngram=False)\n",
    "\n",
    "    with open(f'hashing_vectorizer_origin.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "\n",
    "    ngram_name = vectorizer.transform(df_pred['Name'])\n",
    "    ngram_name_df = pd.DataFrame(ngram_name.toarray())\n",
    "    df_pred = pd.concat([df_pred, ngram_name_df], axis=1)\n",
    "    return df_pred\n",
    "\n",
    "def predict_one_o(df, model):\n",
    "    df.drop(columns=['Name'],inplace=True)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    return model.predict(df)\n",
    "    \n",
    "def create_and_predict_origin(df, model):\n",
    "    df_save = df.copy()\n",
    "    df = feature_creation_o(df)\n",
    "    pred = predict_one_o(df, model)\n",
    "    df['Name'] = df_save\n",
    "    df['Ethnicity'] = pred\n",
    "\n",
    "    return df[['Name', 'Ethnicity']]  # Return a DataFrame with Name and Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = create_and_predict_origin(df_names, predict_origin)\n",
    "df_prediction.to_csv('data/movie_character_ethnicity.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
